{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the SDK to create indexer, data source, etc...\n",
    "\n",
    "The notebook indexer.ipynb used the REST API to create everything. Now we will use the SDK.\n",
    "\n",
    "Be aware that this notebook uses a pre-release version of the SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /Users/geertbaeke/projects/vision/whl/azure_search_documents-11.4.0b12-py3-none-any.whl\n",
      "Requirement already satisfied: azure-core<2.0.0,>=1.24.0 in /opt/homebrew/lib/python3.11/site-packages (from azure-search-documents==11.4.0b12) (1.29.5)\n",
      "Requirement already satisfied: azure-common~=1.1 in /opt/homebrew/lib/python3.11/site-packages (from azure-search-documents==11.4.0b12) (1.1.28)\n",
      "Requirement already satisfied: isodate>=0.6.0 in /opt/homebrew/lib/python3.11/site-packages (from azure-search-documents==11.4.0b12) (0.6.1)\n",
      "Requirement already satisfied: requests>=2.18.4 in /opt/homebrew/lib/python3.11/site-packages (from azure-core<2.0.0,>=1.24.0->azure-search-documents==11.4.0b12) (2.31.0)\n",
      "Requirement already satisfied: six>=1.11.0 in /opt/homebrew/lib/python3.11/site-packages (from azure-core<2.0.0,>=1.24.0->azure-search-documents==11.4.0b12) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /opt/homebrew/lib/python3.11/site-packages (from azure-core<2.0.0,>=1.24.0->azure-search-documents==11.4.0b12) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.24.0->azure-search-documents==11.4.0b12) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.24.0->azure-search-documents==11.4.0b12) (3.5)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.24.0->azure-search-documents==11.4.0b12) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.24.0->azure-search-documents==11.4.0b12) (2023.11.17)\n",
      "azure-search-documents is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n"
     ]
    }
   ],
   "source": [
    "! pip3 install ../whl/azure_search_documents-11.4.0b12-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the indexer\n",
    "\n",
    "Below, there is a full example of creating an index, configuring a semantic profile, two vector profiles, and setting the vector fields to one of the profiles (Hnsw).\n",
    "\n",
    "The integrated vectorizer is also defined.\n",
    "\n",
    "We also use create/update for the indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.core.credentials import AzureKeyCredential  \n",
    "from azure.search.documents import SearchClient  \n",
    "from azure.search.documents.indexes import SearchIndexClient, SearchIndexerClient\n",
    "from azure.search.documents.models import (\n",
    "    QueryAnswerType,\n",
    "    QueryCaptionType,\n",
    "    QueryLanguage,\n",
    "    QueryType,\n",
    "    RawVectorQuery,\n",
    "    VectorizableTextQuery,\n",
    "    VectorFilterMode\n",
    ")\n",
    "from azure.search.documents.indexes.models import (  \n",
    "    AzureOpenAIEmbeddingSkill,  \n",
    "    AzureOpenAIParameters,  \n",
    "    AzureOpenAIVectorizer,  \n",
    "    ExhaustiveKnnParameters,  \n",
    "    ExhaustiveKnnVectorSearchAlgorithmConfiguration,\n",
    "    FieldMapping,  \n",
    "    HnswParameters,  \n",
    "    HnswVectorSearchAlgorithmConfiguration,  \n",
    "    IndexProjectionMode,  \n",
    "    InputFieldMappingEntry,  \n",
    "    OutputFieldMappingEntry,  \n",
    "    PrioritizedFields,    \n",
    "    SearchField,  \n",
    "    SearchFieldDataType,\n",
    "    IndexingParameters,\n",
    "    FieldMappingFunction,\n",
    "    SearchIndex,  \n",
    "    SearchIndexer,  \n",
    "    SearchIndexerDataContainer,  \n",
    "    SearchIndexerDataSourceConnection,  \n",
    "    SearchIndexerIndexProjectionSelector,  \n",
    "    SearchIndexerIndexProjections,  \n",
    "    SearchIndexerIndexProjectionsParameters,\n",
    "    SearchIndexerSkillset,  \n",
    "    SemanticConfiguration,  \n",
    "    SemanticField,  \n",
    "    SemanticSettings,\n",
    "    SimpleField,\n",
    "    SplitSkill,\n",
    "    WebApiSkill,\n",
    "    VectorSearch,  \n",
    "    VectorSearchAlgorithmKind,  \n",
    "    VectorSearchAlgorithmMetric,  \n",
    "    VectorSearchProfile,  \n",
    ")  \n",
    "\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"../.env\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index created or updated successfully\n"
     ]
    }
   ],
   "source": [
    "def blog_index(name: str):\n",
    "\n",
    "    fields = [\n",
    "        SearchField(name=\"path\", type=SearchFieldDataType.String, key=True),\n",
    "        SearchField(name=\"name\", type=SearchFieldDataType.String),\n",
    "        SearchField(name=\"url\", type=SearchFieldDataType.String),\n",
    "        SearchField(name=\"description\", type=SearchFieldDataType.String),\n",
    "        # enrichments will be dumped in enriched field if present; should show vectors & description\n",
    "        SimpleField(name=\"enriched\", type=SearchFieldDataType.String, searchable=False),  # debugging only\n",
    "        SearchField(\n",
    "            name=\"imageVector\",\n",
    "            type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "            searchable=True,\n",
    "            vector_search_dimensions=1024,\n",
    "            vector_search_profile=\"myHnswProfile\"\n",
    "\n",
    "        ),\n",
    "        SearchField(\n",
    "            name=\"textVector\",\n",
    "            type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "            searchable=True,\n",
    "            vector_search_dimensions=1536,\n",
    "            vector_search_profile=\"myHnswProfile\"\n",
    "        ),\n",
    "\n",
    "    ]\n",
    "\n",
    "    vector_config = VectorSearch(  \n",
    "        algorithms=[  \n",
    "            HnswVectorSearchAlgorithmConfiguration(  \n",
    "                name=\"myHnsw\",  \n",
    "                kind=VectorSearchAlgorithmKind.HNSW,  \n",
    "                parameters=HnswParameters(  \n",
    "                    m=4,  \n",
    "                    ef_construction=400,  \n",
    "                    ef_search=500,  \n",
    "                    metric=VectorSearchAlgorithmMetric.COSINE,  \n",
    "                ),  \n",
    "            ),  \n",
    "            ExhaustiveKnnVectorSearchAlgorithmConfiguration(  \n",
    "                name=\"myExhaustiveKnn\",  \n",
    "                kind=VectorSearchAlgorithmKind.EXHAUSTIVE_KNN,  \n",
    "                parameters=ExhaustiveKnnParameters(  \n",
    "                    metric=VectorSearchAlgorithmMetric.COSINE,  \n",
    "                ),  \n",
    "            ),  \n",
    "        ],  \n",
    "        profiles=[  \n",
    "            VectorSearchProfile(  \n",
    "                name=\"myHnswProfile\",  \n",
    "                algorithm=\"myHnsw\",  \n",
    "                vectorizer=\"myOpenAI\",  \n",
    "            ),  \n",
    "            VectorSearchProfile(  \n",
    "                name=\"myExhaustiveKnnProfile\",  \n",
    "                algorithm=\"myExhaustiveKnn\",  \n",
    "                vectorizer=\"myOpenAI\",  \n",
    "            ),  \n",
    "        ],  \n",
    "        vectorizers=[  \n",
    "            AzureOpenAIVectorizer(  \n",
    "                name=\"myOpenAI\",  \n",
    "                kind=\"azureOpenAI\",  \n",
    "                azure_open_ai_parameters=AzureOpenAIParameters(  \n",
    "                    resource_uri=\"https://oa-geba-france.openai.azure.com\",  \n",
    "                    deployment_id=\"embedding\",  \n",
    "                    api_key=os.getenv('AZURE_OPENAI_KEY'),  \n",
    "                ),  \n",
    "            ),  \n",
    "        ],  \n",
    "    )\n",
    "\n",
    "    semantic_config = SemanticConfiguration(  \n",
    "        name=\"my-semantic-config\",  \n",
    "        prioritized_fields=PrioritizedFields(  \n",
    "            prioritized_content_fields=[SemanticField(field_name=\"description\")]  \n",
    "        ),  \n",
    "    )\n",
    "\n",
    "    semantic_settings = SemanticSettings(configurations=[semantic_config])\n",
    "\n",
    "    return SearchIndex(name=name, fields=fields, vector_search=vector_config, semantic_settings=semantic_settings)\n",
    "\n",
    "service_endpoint = \"https://acs-geba.search.windows.net\"\n",
    "index_name = \"images-sdk\"\n",
    "key = os.getenv(\"AZURE_AI_SEARCH_KEY\")\n",
    "\n",
    "\n",
    "index_client = SearchIndexClient(service_endpoint, AzureKeyCredential(key))\n",
    "search_client = SearchClient(service_endpoint, index_name, AzureKeyCredential(key))\n",
    "index = blog_index(index_name)\n",
    "\n",
    "# create the index\n",
    "try:\n",
    "    index_client.create_or_update_index(index)\n",
    "    print(\"Index created or updated successfully\")\n",
    "except Exception as e:\n",
    "    print(\"Index creation error\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the datasource\n",
    "\n",
    "The datasource is created with the SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data source 'images-sdk-blob' created or updated\n"
     ]
    }
   ],
   "source": [
    "# Create a data source \n",
    "ds_client = SearchIndexerClient(service_endpoint, AzureKeyCredential(key))\n",
    "container = SearchIndexerDataContainer(name=\"images\")\n",
    "data_source_connection = SearchIndexerDataSourceConnection(\n",
    "    name=f\"{index_name}-blob\",\n",
    "    type=\"azureblob\",\n",
    "    connection_string=os.getenv(\"STORAGE_CONNNECTION_STRING\"),\n",
    "    container=container\n",
    ")\n",
    "data_source = ds_client.create_or_update_data_source_connection(data_source_connection)\n",
    "\n",
    "print(f\"Data source '{data_source.name}' created or updated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skillsets \n",
    "\n",
    "Now we create the skillset and two skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skillset 'images-sdk-skillset' created or updated\n"
     ]
    }
   ],
   "source": [
    "skillset_name = f\"{index_name}-skillset\"\n",
    "\n",
    "embedding_skill = AzureOpenAIEmbeddingSkill(  \n",
    "    description=\"Skill to generate embeddings via Azure OpenAI\",  \n",
    "    context=\"/document\",  \n",
    "    resource_uri=\"https://oa-geba-france.openai.azure.com\",  \n",
    "    deployment_id=\"embedding\",  \n",
    "    api_key=os.getenv('AZURE_OPENAI_KEY'),  \n",
    "    inputs=[  \n",
    "        InputFieldMappingEntry(name=\"text\", source=\"/document/description\"),  \n",
    "    ],  \n",
    "    outputs=[  \n",
    "        OutputFieldMappingEntry(name=\"embedding\", target_name=\"textVector\")  \n",
    "    ],  \n",
    ")\n",
    "\n",
    "custom_skill = WebApiSkill(\n",
    "    description=\"A custom skill that creates an image vector and description\",\n",
    "    uri=\"https://myskill.gentlebay-4474176e.westeurope.azurecontainerapps.io/vectorize\",\n",
    "    http_method=\"POST\",\n",
    "    timeout=\"PT60S\",\n",
    "    batch_size=4,\n",
    "    degree_of_parallelism=4,\n",
    "    context=\"/document\",\n",
    "    inputs=[\n",
    "        InputFieldMappingEntry(name=\"url\", source=\"/document/url\"),\n",
    "    ],\n",
    "    outputs=[\n",
    "        OutputFieldMappingEntry(name=\"embedding\", target_name=\"imageVector\"),\n",
    "        OutputFieldMappingEntry(name=\"description\", target_name=\"description\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "skillset = SearchIndexerSkillset(  \n",
    "    name=skillset_name,  \n",
    "    description=\"Skillset to chunk documents and generating embeddings\",  \n",
    "    skills=[embedding_skill, custom_skill],  \n",
    ")\n",
    "\n",
    "client = SearchIndexerClient(service_endpoint, AzureKeyCredential(key))\n",
    "client.create_or_update_skillset(skillset)\n",
    "print(f\"Skillset '{skillset.name}' created or updated\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create indexer with SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer_name = f\"{index_name}-indexer\"\n",
    "\n",
    "indexer = SearchIndexer(  \n",
    "    name=indexer_name,  \n",
    "    description=\"Indexer to index documents and generate description and embeddings\",  \n",
    "    skillset_name=skillset_name,  \n",
    "    target_index_name=index_name,\n",
    "    parameters=IndexingParameters(\n",
    "        max_failed_items=-1\n",
    "    ),\n",
    "    data_source_name=data_source.name,  \n",
    "    # Map the metadata_storage_name field to the title field in the index to display the PDF title in the search results  \n",
    "    field_mappings=[\n",
    "        FieldMapping(source_field_name=\"metadata_storage_path\", target_field_name=\"path\", \n",
    "            mapping_function=FieldMappingFunction(name=\"base64Encode\")),\n",
    "        FieldMapping(source_field_name=\"metadata_storage_name\", target_field_name=\"name\"),\n",
    "        FieldMapping(source_field_name=\"metadata_storage_path\", target_field_name=\"url\"),\n",
    "    ],\n",
    "    output_field_mappings=[\n",
    "        FieldMapping(source_field_name=\"/document/textVector\", target_field_name=\"textVector\"),\n",
    "        FieldMapping(source_field_name=\"/document/imageVector\", target_field_name=\"imageVector\"),\n",
    "        FieldMapping(source_field_name=\"/document/description\", target_field_name=\"description\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "indexer_client = SearchIndexerClient(service_endpoint, AzureKeyCredential(key))  \n",
    "indexer_result = indexer_client.create_or_update_indexer(indexer)  \n",
    "  \n",
    "# Run the indexer  \n",
    "# indexer_client.run_indexer(indexer_name)  \n",
    "# print(f' {indexer_name} created')  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector search\n",
    "\n",
    "This search uses the integrated vectorizer which is new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "city.jpg\n",
      "This is an image of the London skyline, featuring a mix of modern skyscrapers and historical buildings. Prominent among the skyscrapers are the Leadenhall Building, also known as the \"Cheesegrater,\" and the rounded, distinctive shape of 30 St Mary Axe, commonly referred to as \"The Gherkin.\" Further in the background, the towers of Canary Wharf can be seen. The view is clear and taken on a day with excellent visibility.\n",
      "https://stgebaoai883.blob.core.windows.net/images/city.jpg\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pure Vector Search\n",
    "query = \"city\"  \n",
    "  \n",
    "search_client = SearchClient(service_endpoint, index_name, credential=AzureKeyCredential(key))\n",
    "vector_query = VectorizableTextQuery(text=query, k=1, fields=\"textVector\", exhaustive=True)\n",
    "# Use the below query to pass in the raw vector query instead of the query vectorization\n",
    "# vector_query = RawVectorQuery(vector=generate_embeddings(query), k=3, fields=\"vector\")\n",
    "  \n",
    "results = search_client.search(  \n",
    "    search_text=None,  \n",
    "    vector_queries= [vector_query],\n",
    "    select=[\"name\", \"description\", \"url\"],\n",
    "    top=1\n",
    ")  \n",
    "\n",
    "# print selected fields from dictionary\n",
    "for result in results:\n",
    "    print(result[\"name\"])\n",
    "    print(result[\"description\"])\n",
    "    print(result[\"url\"])\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "city.jpg\n",
      "This is an image of the London skyline, featuring a mix of modern skyscrapers and historical buildings. Prominent among the skyscrapers are the Leadenhall Building, also known as the \"Cheesegrater,\" and the rounded, distinctive shape of 30 St Mary Axe, commonly referred to as \"The Gherkin.\" Further in the background, the towers of Canary Wharf can be seen. The view is clear and taken on a day with excellent visibility.\n",
      "https://stgebaoai883.blob.core.windows.net/images/city.jpg\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hybrid Search\n",
    "query = \"city\"  \n",
    "  \n",
    "search_client = SearchClient(service_endpoint, index_name, credential=AzureKeyCredential(key))\n",
    "vector_query = VectorizableTextQuery(text=query, k=1, fields=\"textVector\", exhaustive=True)\n",
    "# Use the below query to pass in the raw vector query instead of the query vectorization\n",
    "# vector_query = RawVectorQuery(vector=generate_embeddings(query), k=3, fields=\"vector\")\n",
    "  \n",
    "results = search_client.search(  \n",
    "    search_text=query,  \n",
    "    vector_queries= [vector_query],\n",
    "    search_fields=[\"name\", \"description\"],\n",
    "    select=[\"name\", \"description\", \"url\"],\n",
    "    top=1\n",
    ")  \n",
    "\n",
    "# print selected fields from dictionary\n",
    "for result in results:\n",
    "    print(result[\"name\"])\n",
    "    print(result[\"description\"])\n",
    "    print(result[\"url\"])\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic Hybrid Search\n",
    "query = \"City Skyline\"\n",
    "\n",
    "search_client = SearchClient(service_endpoint, index_name, AzureKeyCredential(key))\n",
    "vector_query = VectorizableTextQuery(text=query, k=2, fields=\"vector\", exhaustive=True)\n",
    "\n",
    "results = search_client.search(  \n",
    "    search_text=query,\n",
    "    vector_queries=[vector_query],\n",
    "    select=[\"parent_id\", \"chunk_id\", \"chunk\"],\n",
    "    query_type=QueryType.SEMANTIC, query_language=QueryLanguage.EN_US, semantic_configuration_name='my-semantic-config', query_caption=QueryCaptionType.EXTRACTIVE, query_answer=QueryAnswerType.EXTRACTIVE,\n",
    "    top=2\n",
    ")\n",
    "\n",
    "semantic_answers = results.get_answers()\n",
    "for answer in semantic_answers:\n",
    "    if answer.highlights:\n",
    "        print(f\"Semantic Answer: {answer.highlights}\")\n",
    "    else:\n",
    "        print(f\"Semantic Answer: {answer.text}\")\n",
    "    print(f\"Semantic Answer Score: {answer.score}\\n\")\n",
    "\n",
    "for result in results:\n",
    "    print(f\"parent_id: {result['parent_id']}\")  \n",
    "    print(f\"chunk_id: {result['chunk_id']}\")  \n",
    "    print(f\"Reranker Score: {result['@search.reranker_score']}\")\n",
    "    print(f\"Content: {result['chunk']}\")  \n",
    "\n",
    "    captions = result[\"@search.captions\"]\n",
    "    if captions:\n",
    "        caption = captions[0]\n",
    "        if caption.highlights:\n",
    "            print(f\"Caption: {caption.highlights}\\n\")\n",
    "        else:\n",
    "            print(f\"Caption: {caption.text}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
